#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–π LLM
"""
import os
import sys
import requests
from dotenv import load_dotenv
from llm_analyzer import LLMAnalyzer
from config import LLMConfig

def test_llm_connection():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–π LLM"""
    print("üß™ –¢–ï–°–¢ –ü–û–î–ö–õ–Æ–ß–ï–ù–ò–Ø –ö –ö–û–†–ü–û–†–ê–¢–ò–í–ù–û–ô LLM")
    print("=" * 50)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    load_dotenv()
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    llm_config = LLMConfig(
        proxy_token=os.getenv('LLM_PROXY_TOKEN', ''),
        base_url=os.getenv('LLM_BASE_URL', 'https://llm.1bitai.ru'),
        model=os.getenv('LLM_MODEL', 'llama3.1:8b-instruct-fp16')
    )
    
    print(f"üìù –ù–∞—Å—Ç—Ä–æ–π–∫–∏:")
    print(f"   URL: {llm_config.base_url}")
    print(f"   –ú–æ–¥–µ–ª—å: {llm_config.model}")
    print(f"   –¢–æ–∫–µ–Ω: {'‚úÖ –µ—Å—Ç—å' if llm_config.proxy_token else '‚ùå –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}")
    print()
    
    if not llm_config.proxy_token:
        print("‚ùå –û–®–ò–ë–ö–ê: –ù–µ –∑–∞–¥–∞–Ω LLM_PROXY_TOKEN –≤ .env —Ñ–∞–π–ª–µ")
        print("–î–æ–±–∞–≤—å—Ç–µ –≤ .env —Ñ–∞–π–ª:")
        print("LLM_PROXY_TOKEN=your_token_here")
        return False
    
    # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    analyzer = LLMAnalyzer(llm_config)
    
    # –¢–µ—Å—Ç 1: –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π
    print("üîç –¢–µ—Å—Ç 1: –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
    try:
        models_url = f"{llm_config.base_url}/v1/models"
        print(f"üåê URL: {models_url}")
        headers = {
            'X-PROXY-AUTH': llm_config.proxy_token,
            'Content-Type': 'application/json',
            'User-Agent': 'Python-LLM-Test/1.0'
        }
        print(f"üìã –ó–∞–≥–æ–ª–æ–≤–∫–∏: {headers}")
        
        response = requests.get(models_url, headers=headers, timeout=30)
        
        print(f"üìä –°—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞: {response.status_code}")
        print(f"üìä –ó–∞–≥–æ–ª–æ–≤–∫–∏ –æ—Ç–≤–µ—Ç–∞: {dict(response.headers)}")
        
        if response.status_code == 200:
            models_data = response.json()
            models = [model['id'] for model in models_data.get('data', [])]
            print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(models)} –º–æ–¥–µ–ª–µ–π")
            print(f"üìã –ü–µ—Ä–≤—ã–µ 5 –º–æ–¥–µ–ª–µ–π: {', '.join(models[:5])}")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π HTTP {response.status_code}")
            print(f"üìÑ –û—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞: {response.text}")
            print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π: {str(e)}")
    
    print()
    
    # –¢–µ—Å—Ç 2: –ó–∞–ø—Ä–æ—Å –∫ —á–∞—Ç—É
    print("üîç –¢–µ—Å—Ç 2: –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –∫ LLM")
    try:
        chat_url = f"{llm_config.base_url}/api/chat"
        
        payload = {
            "model": llm_config.model,
            "stream": False,
            "messages": [
                {
                    "role": "user",
                    "content": "–ü—Ä–∏–≤–µ—Ç! –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ: —Å–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 2+2?"
                }
            ]
        }
        
        print(f"üì° –û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å –∫ LLM: {chat_url}")
        print(f"ü§ñ –ú–æ–¥–µ–ª—å: {llm_config.model}")
        print(f"üìã –ó–∞–≥–æ–ª–æ–≤–∫–∏: {headers}")
        print(f"üì¶ Payload: {payload}")
        
        response = requests.post(
            chat_url,
            headers=headers,
            json=payload,
            timeout=30
        )
        
        print(f"üìä –°—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞: {response.status_code}")
        print(f"üìä –ó–∞–≥–æ–ª–æ–≤–∫–∏ –æ—Ç–≤–µ—Ç–∞: {dict(response.headers)}")
        
        if response.status_code == 200:
            response_data = response.json()
            content = response_data.get('message', {}).get('content', '')
            
            if content:
                print(f"‚úÖ –£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç: {content}")
                return True
            else:
                print("‚ùå –ü–æ–ª—É—á–µ–Ω –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç LLM")
                print(f"üìÑ –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç: {response_data}")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ HTTP {response.status_code}: {response.text}")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ LLM: {str(e)}")
    
    print("‚ùå –ü–æ–ª—É—á–µ–Ω –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç LLM")
    return False

if __name__ == "__main__":
    try:
        success = test_llm_connection()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nüõë –¢–µ—Å—Ç—ã –ø—Ä–µ—Ä–≤–∞–Ω—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1) 